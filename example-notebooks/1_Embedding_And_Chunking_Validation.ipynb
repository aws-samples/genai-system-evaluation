{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7a6cf5-ab59-40d5-baf2-02b03ce4589e",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to demonstrate how to evaluate an embedding model as well as how to evaluate your chunking strategy. Typically, you get better performance boosts out of picking a solid chunking strategy over swapping out an embedding model. In this notebook, we will demonstrate both. \n",
    "\n",
    "# Background\n",
    "Before evaluating an embedding model, it’s important to understand “What” we’re using an embedding model for. The most popular public benchmark is the Massive Text Embedding Benchmark or MTEB. HuggingFace maintains a leaderboard to compare general purpose embedding models against each other to see how they stack up against a wide range of tasks. \n",
    "\n",
    "This is a decent starting place, but you have to ask yourself, how well does this dataset compliment the task I really care about. If I’m creating a RAG Solution for Lawyers, I’m much more interested in how well the embedding model works for comparing legal text vs. how well it works for medical text. This is why it’s important to build out your own evaluation. A model that might not rank high on a general-purpose benchmark, could rank very high on your specific use case. If none of them work very well, then you can make a case for fine tuning an existing model on your data.\n",
    "\n",
    "**What Metrics Should You Care About?**\n",
    "To answer this question, we need to understand what we’re using the embedding model for. For an information retrieval use case, we care about different metrics than we would for a clustering use case. Because retrieval tends to be the most important use case in RAG, lets focus on retrieval.\n",
    "\n",
    "Classic methods apply here to embeddings model like recall@k, precision@k, etc..\n",
    "\n",
    "**How to Evaluate**\n",
    "To perform this evaluation, you need to set up a retrieval task. Generate vector representations of items (documents or chunks) in a shared semantic space and perform a K-nearest-neighbor search on them using a similarity measure (e.g. cosine-similarity, dot-product). This gives you the top-k retrieved item for each query.\n",
    "\n",
    "You need a set of relevance judgments that indicate which documents are relevant to each query. These are typically created by human annotators or derived from click data in product systems.\n",
    "\n",
    "For each query, count the number of relevant items in the top-k retrieved results. Calculate the precision using (number of relevant documents / k). Average the precision values across all queries. \n",
    "Apply these same techniques to other metrics like recall, NDCG, or MAP for a more comprehensive evaluation.\n",
    "\n",
    "## How to create relevance judgements? \n",
    "This is a pretty manual process. For this example, I pasted large chunks of the Opensearch documentation into Claude and asked Claude to come up with a couple example questions about the context. I find it easier to build a validation set where the answer corresponds to 1 to 3 pages. You'll likely tweak your chunking strategy over time, but the relative file paths will stay constant so you don't have to redo your validation dataset every time you make a change to the chunks. \n",
    "\n",
    "\n",
    "# What Will We Do? \n",
    "* We will start with a basic sentence splitting chunking strategy, create embeddings for them, and store them in an in memory vector store (chromaDB).\n",
    "* We will then use our evaluation dataset (which I already created) to run multiple experiments using different embedding models and chunking strategies to see which gives us the best results based on our metrics. \n",
    "\n",
    "**Lets get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29588ea7-23a0-420d-8d27-6cf5f73f3e88",
   "metadata": {},
   "source": [
    "# Initialize clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98973b30-6269-4073-a4ba-29880921f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client from our persisted store\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma\")\n",
    "\n",
    "# Also initialize the bedrock client so we can call some embedding models!\n",
    "session = boto3.Session(profile_name='default')\n",
    "bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ef785-c919-4eb9-83bb-1862572d619a",
   "metadata": {},
   "source": [
    "# Start Running Experiments!\n",
    "\n",
    "## Experiment 1\n",
    "In this first experiment we're going to set up a retrieval task using ChromaDB, Titan Text V1 as our embedding mode, and use very small chunks.\n",
    "\n",
    "\n",
    "#### Word of Caution: On Using GenAI Frameworks\n",
    "For chunking, we'll use LlamaIndex. There are many tools/frameworks for ingesting documents and implementing chunking strategies. I personally like LlamaIndex because it offers a lot of advanced chunking options. It creates \"nodes\" that can be converted into different formats for ingestion.\n",
    "\n",
    "When using these GenAI frameworks, it's best to not rely too heavily on them. In the example below, we'll use LlamaIndex but we'll wrap the chunking logic in a class and normalize the output to a class that we create named RAGChunk. This way, we aren't too reliant on the framework. None of these frameworks are particularly \"stable\" (as of 09/2024) and newer versions are often times not backwards compatible. \n",
    "\n",
    "It's best to contain the package to minimize the impact on the rest of your system if/when you need to have more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8003fb-15ea-4995-9859-6f0f0ba0b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "import re\n",
    "\n",
    "# Create a class to use instead of LlamaIndex Nodes. This way we decouple our chroma collections from LlamaIndexes\n",
    "class RAGChunk(BaseModel):\n",
    "    id_: str\n",
    "    text: str\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "class SentenceSplitterChunkingStrategy:\n",
    "    def __init__(self, input_dir: str, chunk_size: int = 256, chunk_overlap: int = 128):\n",
    "        self.input_dir = input_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "        # Helper to get regex pattern for normalizing relative file paths.\n",
    "        self.relative_path_pattern = rf\"{re.escape(input_dir)}(/.*)\"\n",
    "\n",
    "    def _extract_relative_path(self, full_path):\n",
    "        # Get Regex pattern\n",
    "        pattern = self.relative_path_pattern\n",
    "        match = re.search(pattern, full_path)\n",
    "        if match:\n",
    "            return match.group(1).lstrip('/')\n",
    "        return None\n",
    "\n",
    "    def _create_pipeline(self) -> IngestionPipeline:\n",
    "        transformations = [\n",
    "            SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap),\n",
    "        ]\n",
    "        return IngestionPipeline(transformations=transformations)\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        # If you're using a different type of file besides md, you'll want to change this. \n",
    "        return SimpleDirectoryReader(\n",
    "            input_dir=self.input_dir, \n",
    "            recursive=True,\n",
    "            required_exts=['.md']\n",
    "        ).load_data()\n",
    "\n",
    "    def to_ragchunks(self, nodes: List[Node]) -> List[RAGChunk]:\n",
    "        return [\n",
    "            RAGChunk(\n",
    "                id_=node.node_id,\n",
    "                text=node.text,\n",
    "                metadata={\n",
    "                    **node.metadata,\n",
    "                    'relative_path': self._extract_relative_path(node.metadata['file_path'])\n",
    "                }\n",
    "            )\n",
    "            for node in nodes\n",
    "        ]\n",
    "\n",
    "    def process(self) -> List[RAGChunk]:\n",
    "        documents = self.load_documents()\n",
    "        nodes = self.pipeline.run(documents=documents)\n",
    "        rag_chunks = self.to_ragchunks(nodes)\n",
    "        \n",
    "        print(f\"Processing complete. Created {len(rag_chunks)} chunks.\")\n",
    "        return rag_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb1bf3-4002-45e6-8733-e047b399e526",
   "metadata": {},
   "source": [
    "## Create Chunks\n",
    "In this step we'll use a custom wrapper we built around LlamaIndex. It will split up documents from the input dir into ~512 chunk sizes with the overlap (or smaller if the file isn't that big). We should get around ~16k chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265cdac-dc1b-40bb-90c2-ad5f0f6f5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_strategy = SentenceSplitterChunkingStrategy(\n",
    "    input_dir=\"../data/opensearch-docs/documentation-website\",\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Get the nodes from the chunker.\n",
    "chunks: RAGChunk = chunking_strategy.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5324adb-df27-4fcf-9ae9-b127c2e6f73e",
   "metadata": {},
   "source": [
    "### Setup Retrieval Task\n",
    "The next step is to set up a retrieval task. To do this, we'll use chromaDB as our vector database. We've built a wrapper around the retrieval task and created a BaseRetrievalTask class to inherit from. If you'd like to experiment with more complicated retrieval pattern, you can write your own implementation and the rest of the notebook will run accordingly. \n",
    "\n",
    "We'll also leverage Chromas feature that allows us to specify an embedding function when creating a collection. This makes ingestion simpler because Chroma will automatically apply the same embedding function to our queries as it did for our documents. It's just nicer to keep the embedding function and DB together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf1505-2ae4-411c-a3c0-7ab20a5ab678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "import chromadb\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "from typing import List, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "# Base retrieval class. Can be reused if you decide to implement a different retrieval class.\n",
    "class BaseRetrievalTask(ABC):\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query_text: str, n_results: int) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Retrieve documents based on the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to search for.\n",
    "\n",
    "        Returns:\n",
    "            List[RetrievalResult]: A list of RetrievalResult objects that are relevant to the query.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Example of a concrete implementation\n",
    "class ChromaDBRetrievalTask(BaseRetrievalTask):\n",
    "\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function, chunks: List[RAGChunk]):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        self.chunks = chunks\n",
    "\n",
    "        # Create the collection\n",
    "        self.collection = self._create_collection()\n",
    "\n",
    "    def _create_collection(self):\n",
    "        return self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def add_chunks_to_collection(self, batch_size: int = 20, num_workers: int = 10):\n",
    "        batches = [self.chunks[i:i + batch_size] for i in range(0, len(self.chunks), batch_size)]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = [executor.submit(self._add_batch, batch) for batch in batches]\n",
    "            for future in as_completed(futures):\n",
    "                future.result()  # This will raise an exception if one occurred during the execution\n",
    "        print('Finished Ingesting Chunks Into Collection')\n",
    "\n",
    "    def _add_batch(self, batch: List[RAGChunk]):\n",
    "        self.collection.add(\n",
    "            ids=[chunk.id_ for chunk in batch],\n",
    "            documents=[chunk.text for chunk in batch],\n",
    "            metadatas=[chunk.metadata for chunk in batch]\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query_text: str, n_results: int = 5) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "\n",
    "        return retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec70972-4873-4f6b-aa86-c317ad987bab",
   "metadata": {},
   "source": [
    "### Populate the vectorDB\n",
    "In the next section we'll define our embedding function and populate the in memory database with our vectors. **Note** We've already indexed the chunks for you which are loaded when retrieving or creating the collection and specifying a persistant chromaDB store. If you'd like to redo it yourself, feel free to, but it does take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015eb494-855c-45d7-8d51-8294aa758a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "# Define some experiment variables\n",
    "TITAN_TEXT_EMBED_V1_ID: str = 'amazon.titan-embed-text-v1'\n",
    "EXPERIMENT_1_COLLECTION_NAME: str = 'experiment_1_collection'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V1_ID\n",
    ")\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same!\n",
    "experiment_1_retrieval_task: BaseRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = EXPERIMENT_1_COLLECTION_NAME,\n",
    "    embedding_function = embedding_function,\n",
    "    chunks = chunks\n",
    ")\n",
    "\n",
    "# If you've already created collection, comment out this line\n",
    "experiment_1_retrieval_task.add_chunks_to_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa46a88-bc03-48a3-85bc-1977f32ebbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify it works!\n",
    "print(len(experiment_1_retrieval_task.retrieve('What does * do?', n_results=1)) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31466f24-ac9f-4cfb-9958-6ffb50e4d710",
   "metadata": {},
   "source": [
    "### Pull In Validation Dataset\n",
    "We've already created a validation dataset for you. Through a combination of human curation & trial and error, we created a set of 25 questions users might ask a RAG system designed to answer questions from OpenSearch documentation. We've also annotated the questions with the relevant relative paths of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b12de-085a-4e19-90f8-d7d508939061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_clean_eval_dataset():\n",
    "    EVAL_PATH = '../data/eval-datasets/1_embeddings_validation.csv'\n",
    "    eval_df = pd.read_csv(EVAL_PATH)\n",
    "\n",
    "    # Clean up the DataFrame\n",
    "    eval_df = eval_df.rename(columns=lambda x: x.strip())  # Remove any leading/trailing whitespace from column names\n",
    "    eval_df = eval_df.drop(columns=[col for col in eval_df.columns if col.startswith('Unnamed')])  # Remove unnamed columns\n",
    "    eval_df = eval_df.dropna(how='all')  # Remove rows that are all NaN\n",
    "    \n",
    "    # Strip whitespace from string columns\n",
    "    for col in eval_df.select_dtypes(['object']):\n",
    "        eval_df[col] = eval_df[col].str.strip()\n",
    "    \n",
    "    # Ensure 'relevant_doc_ids' is a string column\n",
    "    eval_df['relevant_doc_ids'] = eval_df['relevant_doc_ids'].astype(str)\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "eval_df = get_clean_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae59c14-2f8f-4274-8231-318cb3dc1321",
   "metadata": {},
   "source": [
    "### Define Metrics\n",
    "The IRMetricsCalculator below calculates a series of metrics that will be useful when evaluating your RAG system. Remember, we are only evaluating the retrieval at this stage, not the models ability to create an answer from the IR results.\n",
    "\n",
    "#### Metrics\n",
    "* precision@k:\n",
    "* recall@k:\n",
    "* ndcg@k\n",
    "\n",
    "These individual metrics will be our basis for creating an aggregate view of our validation dataset to get a sense for how well it's performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e79d0e-8a07-450f-a3f1-9ded7650b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#  Helper class for calculating metrics.\n",
    "class IRMetricsCalculator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / k if k > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def dcg_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        dcg = 0\n",
    "        for i, item in enumerate(retrieved_k):\n",
    "            if item in relevant:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        return dcg\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant, retrieved, k):\n",
    "        dcg = IRMetricsCalculator.dcg_at_k(relevant, retrieved, k)\n",
    "        idcg = IRMetricsCalculator.dcg_at_k(relevant, relevant, k)\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_json_list(json_string):\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {json_string} with error {e}\")\n",
    "            return []\n",
    "\n",
    "    def calculate_metrics(self, k_values=[1, 3, 5, 10]):\n",
    "        for k in k_values:\n",
    "            self.df[f'precision@{k}'] = self.df.apply(lambda row: self.precision_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'recall@{k}'] = self.df.apply(lambda row: self.recall_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'ndcg@{k}'] = self.df.apply(lambda row: self.ndcg_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40741563-93ec-45fb-b481-5ac2882b146c",
   "metadata": {},
   "source": [
    "# Setup Task Runner\n",
    "In the step below we'll setup a task runner that will iterate through our dataframe, run a retrieval task on the input and use our IRCalculator to generate metrics on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f0aeb-2588-4e60-90fd-6e9c17984ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalTaskRunner:\n",
    "    def __init__(self, eval_df: pd.DataFrame, retrieval_task: BaseRetrievalTask):\n",
    "        self.eval_df = eval_df\n",
    "        self.retrieval_task = retrieval_task\n",
    "\n",
    "    def _get_unique_file_paths(self, results: List[RetrievalResult]) -> List[str]:\n",
    "        # Since Python 3.7, dicts retain insertion order.\n",
    "        return list(dict.fromkeys(r.metadata['relative_path'] for r in results))\n",
    "        \n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        # Make a copy of the dataframe so we don't modify the original.\n",
    "        df = pd.DataFrame(self.eval_df)\n",
    "        \n",
    "        results = []\n",
    "        for index, row in df.iterrows():\n",
    "            query: str = row['query_text']\n",
    "            \n",
    "            # Run retrieval task\n",
    "            retrieval_results: List[RetrievalResult] = self.retrieval_task.retrieve(query)\n",
    "            \n",
    "            # Extract unique page numbers for comparison with validation dataset.\n",
    "            ordered_filepaths: List[str] = self._get_unique_file_paths(retrieval_results)\n",
    "\n",
    "            retrieved_chunks = [ {'relative_path': r.metadata['relative_path'], 'chunk': r.document} for r in retrieval_results ]\n",
    "\n",
    "            # Create new record\n",
    "            result = {\n",
    "                'query_text': query,\n",
    "                'relevant_doc_ids': row['relevant_doc_ids'],\n",
    "                'retrieved_doc_ids': json.dumps(ordered_filepaths),\n",
    "                'retrieved_chunks': json.dumps(retrieved_chunks), # Best way to preserve the chunks\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        new_dataframe = pd.DataFrame(results)\n",
    "        # return new_dataframe\n",
    "\n",
    "        ir_calc: IRMetricsCalculator = IRMetricsCalculator(new_dataframe)\n",
    "        return ir_calc.calculate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675e733-783c-4e44-89a3-5a5bf6fec2fa",
   "metadata": {},
   "source": [
    "### Execute first experiment\n",
    "In the command below, we'll execute our first experiment and store the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf7d38-b4ff-4718-86e1-40ad219cd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_results: pd.DataFrame = RetrievalTaskRunner(eval_df, experiment_1_retrieval_task).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877e71-494a-4d60-9528-66312a7097dc",
   "metadata": {},
   "source": [
    "## Create a Summary\n",
    "You can view the results for each individual query, but it doesn't quite tell the whole story. The last thing we need to do to conclude experiment 1 is to create a summary view showing Mean Average Precision, Mean Reciprocal Rank (MRR), as well as general averages across all the individual metrics we calculated. This should let us know how well the retrieval task is performing.\n",
    "\n",
    "To do that, we've created another helper class to summarize the results and give us a comprehensive view of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17b963-ea87-4527-aac3-e0060268b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class ExperimentSummarizer:\n",
    "    def __init__(self, df):\n",
    "        self.df = pd.DataFrame(df)\n",
    "        self.summary_df = None\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ap(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        relevant_count = 0\n",
    "        total_precision = 0\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                relevant_count += 1\n",
    "                total_precision += relevant_count / i\n",
    "        \n",
    "        return total_precision / len(relevant_set) if relevant_set else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_reciprocal_rank(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                return 1 / i\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def calculate_map(self):\n",
    "        self.df['AP'] = self.df.apply(lambda row: self.calculate_ap(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['AP'].mean()\n",
    "\n",
    "    def calculate_mrr(self):\n",
    "        self.df['RR'] = self.df.apply(lambda row: self.calculate_reciprocal_rank(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['RR'].mean()\n",
    "\n",
    "    def calculate_mean_metrics(self):\n",
    "        return self.df[[\n",
    "            'precision@1', 'recall@1', 'ndcg@1',\n",
    "            'precision@3', 'recall@3', 'ndcg@3',\n",
    "            'precision@5', 'recall@5', 'ndcg@5'\n",
    "        ]].mean()\n",
    "\n",
    "    def calculate_top_k_percentages(self):\n",
    "        top_1 = (self.df['precision@1'] > 0).mean() * 100\n",
    "        top_3 = (self.df['precision@3'] > 0).mean() * 100\n",
    "        top_5 = (self.df['precision@5'] > 0).mean() * 100\n",
    "        return top_1, top_3, top_5\n",
    "\n",
    "    def analyze(self):\n",
    "        map_score = self.calculate_map()\n",
    "        mrr_score = self.calculate_mrr()\n",
    "        mean_metrics = self.calculate_mean_metrics()\n",
    "        top_1, top_3, top_5 = self.calculate_top_k_percentages()\n",
    "\n",
    "        self.summary_df = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'MAP (Mean Average Precision)',\n",
    "                'MRR (Mean Reciprocal Rank)',\n",
    "                'Mean Precision@1', 'Mean Recall@1', 'Mean NDCG@1',\n",
    "                'Mean Precision@3', 'Mean Recall@3', 'Mean NDCG@3',\n",
    "                'Mean Precision@5', 'Mean Recall@5', 'Mean NDCG@5',\n",
    "                '% Queries with Relevant Doc in Top 1',\n",
    "                '% Queries with Relevant Doc in Top 3',\n",
    "                '% Queries with Relevant Doc in Top 5'\n",
    "            ],\n",
    "            'Value': [\n",
    "                map_score,\n",
    "                mrr_score,\n",
    "                mean_metrics['precision@1'], mean_metrics['recall@1'], mean_metrics['ndcg@1'],\n",
    "                mean_metrics['precision@3'], mean_metrics['recall@3'], mean_metrics['ndcg@3'],\n",
    "                mean_metrics['precision@5'], mean_metrics['recall@5'], mean_metrics['ndcg@5'],\n",
    "                top_1, top_3, top_5\n",
    "            ]\n",
    "        })\n",
    "        return self.summary_df\n",
    "\n",
    "    def get_summary(self):\n",
    "        if self.summary_df is None:\n",
    "            self.analyze()\n",
    "        return self.summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e3d69-36e5-4a8b-a00c-3caa8a08a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the class above to create aggregate metrics to see how well the system performs.\n",
    "experiment_1_summary = ExperimentSummarizer(experiment_1_results).analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29b5de-71a9-461f-87a2-eb898b3eb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7f19-d5de-4dc6-8e48-9e87fb6fa7a7",
   "metadata": {},
   "source": [
    "# Takeaways from Experiment 1\n",
    "The results aren't.. terrible. But they could also be a lot better. We plan to add a ReRank step in the next notebook, so while rank based metrics are important at this stage, we care more about whether the top k results have relevant data in them. This makes recall@5 arguably the most important metric to evaluate on in this step.\n",
    "\n",
    "However, all the metrics are important to understand how the base IR task is performing. We want to limit the amount of context we pass back to the model to save on input token cost so knowing precesion@1 and precision@5 give us an idea of how well the embeddings are working on their own at ranking the results. We can then take these runs and compare it against a re-ranked list to see (if) re-rank improves this task.\n",
    "\n",
    "## Next Steps\n",
    "Let's pick larger chunks to see if it performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbadff-7ffb-45b3-886d-4329aa341a9d",
   "metadata": {},
   "source": [
    "# Experiment 2 - Larger Chunk Sizes\n",
    "At 2046 chunk sizes, there's very few documents in our opensearch docs that would be split up. This essentially becomes page level chunking. Lets try it out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f5a75-f4e8-416d-ae10-168317d46b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define some smaller chunks\n",
    "chunking_strategy = SentenceSplitterChunkingStrategy(\n",
    "    input_dir=\"../data/opensearch-docs/documentation-website\",\n",
    "    chunk_size=2046,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Get the nodes from the chunker.\n",
    "chunks: RAGChunk = chunking_strategy.process()\n",
    "\n",
    "# Define some experiment variables\n",
    "TITAN_TEXT_EMBED_V1_ID: str = 'amazon.titan-embed-text-v1'\n",
    "EXPERIMENT_2_COLLECTION_NAME: str = 'experiment_2_collection'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V1_ID\n",
    ")\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same!\n",
    "experiment_2_retrieval_task: BaseRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = EXPERIMENT_2_COLLECTION_NAME,\n",
    "    embedding_function = embedding_function,\n",
    "    chunks = chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9bdb1-d3cd-48fb-8140-a9b04e8ed33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've already created collection, comment out this line\n",
    "experiment_2_retrieval_task.add_chunks_to_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee99914-831b-4a18-ba29-ffac380eefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify it works!\n",
    "print(len(experiment_2_retrieval_task.retrieve('What does * do?', n_results=1)) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fc8a4-4655-428c-92f2-5fc03301725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a new Task Runner for experiment 2\n",
    "experiment_2_results: pd.DataFrame = RetrievalTaskRunner(eval_df, experiment_2_retrieval_task).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993e69e-8d7a-4c05-86b0-7402f103f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the class above to create aggregate metrics to see how well the system performs.\n",
    "experiment_2_summary = ExperimentSummarizer(experiment_2_results).analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc26877-905b-429b-b840-e9204ec85990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_2_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4679ede-c760-49d4-b1d5-7adfcfc7ea5a",
   "metadata": {},
   "source": [
    "## Compare Experiment 2 with Experiment 1\n",
    "The results above look better. However, it's kind of hard to visualize how much better. Lets use another helper class to compare the results between two experiments and pretty print the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22c66e-7a92-47c6-b6c6-621c8e681ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentComparator:\n",
    "    def __init__(self, *experiment_data):\n",
    "        self.experiments = experiment_data\n",
    "\n",
    "    def compare_metrics(self):\n",
    "        merged_df = pd.DataFrame({'Metric': self.experiments[0][0]['Metric']})\n",
    "        for df, name in self.experiments:\n",
    "            merged_df = pd.merge(merged_df, df, on='Metric', how='left')\n",
    "            merged_df = merged_df.rename(columns={'Value': name})\n",
    "        \n",
    "        base_exp = self.experiments[0][1]\n",
    "        for df, name in self.experiments[1:]:\n",
    "            merged_df[f'Change_{name}_vs_{base_exp}'] = merged_df[name] - merged_df[base_exp]\n",
    "            merged_df[f'PercentChange_{name}_vs_{base_exp}'] = ((merged_df[name] - merged_df[base_exp]) / merged_df[base_exp]) * 100\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    def print_comparison(self):\n",
    "        comparison = self.compare_metrics()\n",
    "        \n",
    "        def color_change(val):\n",
    "            if pd.isna(val):\n",
    "                return ''\n",
    "            return 'color: red' if val < 0 else 'color: green' if val > 0 else ''\n",
    "        \n",
    "        def background_color_change(val):\n",
    "            if pd.isna(val):\n",
    "                return ''\n",
    "            return 'background-color: #ffcccb' if val < 0 else 'background-color: #90ee90' if val > 0 else ''\n",
    "        \n",
    "        change_columns = [col for col in comparison.columns if col.startswith('Change_') or col.startswith('PercentChange_')]\n",
    "        styled = comparison.style\n",
    "        \n",
    "        for col in change_columns:\n",
    "            styled = styled.map(color_change, subset=[col])\n",
    "            styled = styled.map(background_color_change, subset=[col])\n",
    "        \n",
    "        numeric_columns = comparison.select_dtypes(include=[np.number]).columns\n",
    "        format_dict = {col: '{:.6f}' for col in numeric_columns}\n",
    "        \n",
    "        for col in change_columns:\n",
    "            if col.startswith('PercentChange_'):\n",
    "                format_dict[col] = '{:.2f}%'\n",
    "        \n",
    "        styled = styled.format(format_dict)\n",
    "        return styled\n",
    "\n",
    "    def analyze(self):\n",
    "        return self.print_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc5a12-2ab9-4619-a69f-e372c992279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_comparator = ExperimentComparator(\n",
    "    (experiment_1_summary, \"Experiment1\"),\n",
    "    (experiment_2_summary, \"Experiment2\")\n",
    ")\n",
    "experiment_comparator.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523a546-dd8c-4220-aa49-344120469643",
   "metadata": {},
   "source": [
    "# Takeaways from Experiment 2\n",
    "Switching to larger chunks actually made the results better. That's why we build validation datasets to understand what works!\n",
    "\n",
    "Diving into the results a bit. It seems like the retrieval in general could use a boost in general. We're using an older version of Titan Text embeddings, lets use a newer version with the same chunk size as experiment 2 and see how that improves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bdafe-ddd4-4d0d-8a51-b6aae29926ad",
   "metadata": {},
   "source": [
    "# Experiment 3 - Titan Embeddings V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe35da-8b77-472c-a2f8-8f07de888ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define some smaller chunks\n",
    "chunking_strategy = SentenceSplitterChunkingStrategy(\n",
    "    input_dir=\"../data/opensearch-docs/documentation-website\",\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Get the nodes from the chunker.\n",
    "chunks: RAGChunk = chunking_strategy.process()\n",
    "\n",
    "# Define some experiment variables\n",
    "TITAN_TEXT_EMBED_V2_ID: str = \"amazon.titan-embed-text-v2:0\"\n",
    "EXPERIMENT_3_COLLECTION_NAME: str = 'experiment_3_collection'\n",
    "\n",
    "# Update our embeddings model to a newer one.\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V2_ID\n",
    ")\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same!\n",
    "experiment_3_retrieval_task: BaseRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = EXPERIMENT_3_COLLECTION_NAME,\n",
    "    embedding_function = embedding_function,\n",
    "    chunks = chunks\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cf385-583f-426d-9d0b-2cc5d68982fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you've already created collection, comment out this line\n",
    "experiment_3_retrieval_task.add_chunks_to_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d17f2-4d32-4ae5-9202-da80529cec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a new Task Runner for experiment 2\n",
    "experiment_3_results: pd.DataFrame = RetrievalTaskRunner(eval_df, experiment_3_retrieval_task).run()\n",
    "# Lets use the class above to create aggregate metrics to see how well the system performs.\n",
    "experiment_3_summary = ExperimentSummarizer(experiment_3_results).analyze()\n",
    "\n",
    "experiment_comparator = ExperimentComparator(\n",
    "    (experiment_2_summary, \"Experiment2\"),\n",
    "    (experiment_3_summary, \"Experiment3\")\n",
    ")\n",
    "experiment_comparator.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b7611-f934-4887-8623-c43be94c738b",
   "metadata": {},
   "source": [
    "# Takeaways from Experiment 3\n",
    "Switching to titan V2 did actually improve our performance significantly! In general these scores are pretty decent. If you're following along, you should see a mean recall@5 at around 78%. The queries with relevant docs in them should be around 87%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3a834-a47a-4456-8286-23744f3d0097",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "By playing with the chunk size and embedding model, we made some very impressive improvements over our first iteration. The numbers shown above are pretty decent. \n",
    "\n",
    "# TODO\n",
    "For those following along, what kind of advanced chunking / embedding strategies can you think of to implement that might get our metrics closer to:\n",
    "\n",
    "* MAP > .8\n",
    "* precision@1 > .7\n",
    "* Recall@5 > .9\n",
    "* NDCG@5 > .8\n",
    "* % of queries with relevant Docs in Top 5 > .9\n",
    "\n",
    "# Takeways\n",
    "We ran 3 quick experiments to determine a good chunk size and compared different embedding models from Amazon. There are many more chunking strategies that offer more advanced capabilities. Hierarchical, Semantic, and summarization chunking strategies can also greatly improve performance. For this notebook, we elected to start with very basic ones. It's worth noting that LlamaIndex's SentenceSplitter (which we used) does its best to keep sentences together.\n",
    "\n",
    "# Next Steps\n",
    "Move over to the ReRank notebook to see how ReRank could improve our metrics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
